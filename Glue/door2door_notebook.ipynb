{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::719386081370:role/role_glue_s3\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: f93ecaa1-fa5f-4dba-b643-edcdca2889fa\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session f93ecaa1-fa5f-4dba-b643-edcdca2889fa to get into ready status...\nSession f93ecaa1-fa5f-4dba-b643-edcdca2889fa has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.utils import getResolvedOptions\n\nargs = getResolvedOptions(sys.argv,\n                          ['JOB_NAME',\n                           'day'])",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "import boto3\n\ns3 = boto3.client('s3')\nbucket_name = 'door2door-de'\nprefix = 'data/' + args['day']\nobjects = []\n\n# Paginate over the objects in the S3 bucket\npaginator = s3.get_paginator('list_objects_v2')\nfor page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n    if 'Contents' in page:\n        for obj in page['Contents']:\n            if obj['Key'] != prefix:\n                # Append the object's key to the list of objects\n                objects.append('s3://' + bucket_name + '/' + obj['Key'])",
			"metadata": {
				"trusted": true
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf = glueContext.create_dynamic_frame.from_options(\n    's3',\n    {'paths': objects},\n    format='json'\n)\ndyf.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- event: string\n|-- on: string\n|-- at: string\n|-- data: struct\n|    |-- id: string\n|    |-- location: struct\n|    |    |-- lat: double\n|    |    |-- lng: double\n|    |    |-- at: string\n|    |-- start: string\n|    |-- finish: string\n|-- organization_id: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# dyf = glueContext.create_dynamic_frame.from_catalog(database='door2door', table_name='data')\n# dyf.printSchema()",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- event: string\n|-- on: string\n|-- at: string\n|-- data: struct\n|    |-- id: string\n|    |-- location: struct\n|    |    |-- lat: double\n|    |    |-- lng: double\n|    |    |-- at: string\n|    |-- start: string\n|    |-- finish: string\n|-- organization_id: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### 1. Separate the DynamicFrame according to the event\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# Filter the input DynamicFrame based on the \"on\" column\nvehicle_dyf = dyf.filter(lambda x: x[\"on\"] == \"vehicle\")\nperiod_dyf = dyf.filter(lambda x: x[\"on\"] == \"operating_period\")",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### a. Mapping vehicle",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "# Define the mapping to extract fields from the \"data\" struct\nmapping_vehicle = [\n    (\"data.id\", \"string\", \"id\", \"string\"),\n    (\"data.location.lat\", \"double\", \"lat\", \"double\"),\n    (\"data.location.lng\", \"double\", \"lng\", \"double\"),\n    (\"data.location.at\", \"string\", \"location_at\", \"timestamp\"),\n    (\"event\", \"string\", \"event\", \"string\"),\n]\n\n# Use the ApplyMapping transform to extract fields and create a new DynamicFrame\ndyf_vehicle = ApplyMapping.apply(\n    frame=vehicle_dyf,\n    mappings=mapping_vehicle\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf_vehicle.toDF().show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+--------+--------+--------------------+------+\n|                  id|     lat|     lng|         location_at| event|\n+--------------------+--------+--------+--------------------+------+\n|bac5188f-67c6-496...|52.45133|13.46045|2019-06-01 18:17:...|update|\n|3a3eb23a-f22e-4fe...|52.45848|13.52647|2019-06-01 18:17:...|update|\n|f0b87796-b25c-40b...|52.50309|13.33435|2019-06-01 18:17:...|update|\n|9152c5d8-79cf-4fe...|52.50536|13.51655|2019-06-01 18:17:...|update|\n|f06eb89c-ada0-41c...|52.49697|13.44936|2019-06-01 18:17:...|update|\n|9d6a8840-def2-42b...|52.46324|13.34227|2019-06-01 18:17:...|update|\n|3b0640d6-502d-462...|52.57786|13.26756|2019-06-01 18:17:...|update|\n|98c8b8cb-7c2b-415...|52.50036|13.25032|2019-06-01 18:17:...|update|\n|d6880741-ae7f-474...|52.47874|13.32032|2019-06-01 18:17:...|update|\n|d759fc35-b25c-487...|52.44846|13.46759|2019-06-01 18:17:...|update|\n|49c02de6-2bc6-46c...|52.44493|13.24099|2019-06-01 18:17:...|update|\n|949798fc-50aa-47a...|52.55031|13.51002|2019-06-01 18:17:...|update|\n|e641b45f-f007-4d7...|52.48219|13.25166|2019-06-01 18:17:...|update|\n|b4dac3b4-0142-431...| 52.4743|13.54158|2019-06-01 18:17:...|update|\n|46952066-0a94-4a2...|52.48389| 13.2538|2019-06-01 18:17:...|update|\n|ae413d00-fb27-476...|52.45518|13.47735|2019-06-01 18:17:...|update|\n|a38edd77-07b0-406...|52.44395|13.24243|2019-06-01 18:17:...|update|\n|804ac7f0-cf1d-4b2...|52.44176|13.50779|2019-06-01 18:17:...|update|\n|46f718d4-50de-468...|52.47633|13.45905|2019-06-01 18:17:...|update|\n|8efb00c5-3b7f-424...|  52.483|13.28398|2019-06-01 18:17:...|update|\n+--------------------+--------+--------+--------------------+------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### b. Mapping period",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Define the mapping to extract fields from the \"data\" struct\nmapping_period = [\n    (\"data.id\", \"string\", \"id\", \"string\"),\n    (\"data.start\", \"string\", \"start\", \"timestamp\"),\n    (\"data.finish\", \"string\", \"finish\", \"timestamp\"),\n    (\"event\", \"string\", \"event\", \"string\"),\n]\n\n# Use the ApplyMapping transform to extract fields and create a new DynamicFrame\ndyf_period = ApplyMapping.apply(\n    frame=period_dyf,\n    mappings=mapping_period\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf_period.toDF().show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----+--------------------+--------------------+------+\n|  id|               start|              finish| event|\n+----+--------------------+--------------------+------+\n|op_2|2019-06-01 18:17:...|2019-06-01 18:22:...|create|\n|op_1|2019-06-01 18:23:...|2019-06-01 18:28:...|create|\n+----+--------------------+--------------------+------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### 2. Define the schema for the fact table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import monotonically_increasing_id\n\n\nmapping = [\n    (\"on\", \"string\", \"on\", \"string\"),\n    (\"at\", \"string\", \"date\", \"timestamp\"),\n    (\"data.id\", \"string\", \"data_id\", \"string\"),\n    (\"organization_id\", \"string\", \"organization_id\", \"string\"),\n]\n\n# Use the ApplyMapping transform to add a new column for the \"id\" field\nnew_dyf = ApplyMapping.apply(\n    frame=dyf,\n    mappings=mapping\n)\n\nnew_dyf = new_dyf.withColumn(\"id\", monotonically_increasing_id())",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "new_dyf.toDF().show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+--------------------+--------------------+---------------+---+\n|     on|                date|             data_id|organization_id| id|\n+-------+--------------------+--------------------+---------------+---+\n|vehicle|2019-06-01 18:17:...|bac5188f-67c6-496...|         org-id|  0|\n|vehicle|2019-06-01 18:17:...|3a3eb23a-f22e-4fe...|         org-id|  1|\n|vehicle|2019-06-01 18:17:...|f0b87796-b25c-40b...|         org-id|  2|\n|vehicle|2019-06-01 18:17:...|9152c5d8-79cf-4fe...|         org-id|  3|\n|vehicle|2019-06-01 18:17:...|f06eb89c-ada0-41c...|         org-id|  4|\n|vehicle|2019-06-01 18:17:...|9d6a8840-def2-42b...|         org-id|  5|\n|vehicle|2019-06-01 18:17:...|3b0640d6-502d-462...|         org-id|  6|\n|vehicle|2019-06-01 18:17:...|98c8b8cb-7c2b-415...|         org-id|  7|\n|vehicle|2019-06-01 18:17:...|d6880741-ae7f-474...|         org-id|  8|\n|vehicle|2019-06-01 18:17:...|d759fc35-b25c-487...|         org-id|  9|\n|vehicle|2019-06-01 18:17:...|49c02de6-2bc6-46c...|         org-id| 10|\n|vehicle|2019-06-01 18:17:...|949798fc-50aa-47a...|         org-id| 11|\n|vehicle|2019-06-01 18:17:...|e641b45f-f007-4d7...|         org-id| 12|\n|vehicle|2019-06-01 18:17:...|b4dac3b4-0142-431...|         org-id| 13|\n|vehicle|2019-06-01 18:17:...|46952066-0a94-4a2...|         org-id| 14|\n|vehicle|2019-06-01 18:17:...|ae413d00-fb27-476...|         org-id| 15|\n|vehicle|2019-06-01 18:17:...|a38edd77-07b0-406...|         org-id| 16|\n|vehicle|2019-06-01 18:17:...|804ac7f0-cf1d-4b2...|         org-id| 17|\n|vehicle|2019-06-01 18:17:...|46f718d4-50de-468...|         org-id| 18|\n|vehicle|2019-06-01 18:17:...|8efb00c5-3b7f-424...|         org-id| 19|\n+-------+--------------------+--------------------+---------------+---+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "glueContext.write_dynamic_frame.from_catalog(frame=new_dyf, database=\"door2door\", table_name=\"fact\", transformation_ctx=\"datasink\")\nglueContext.write_dynamic_frame.from_catalog(frame=dyf_vehicle, database=\"door2door\", table_name=\"vehicle\", transformation_ctx=\"datasink\")\nglueContext.write_dynamic_frame.from_catalog(frame=dyf_period, database=\"door2door\", table_name=\"period\", transformation_ctx=\"datasink\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7fcc689a00d0>\n",
					"output_type": "stream"
				}
			]
		}
	]
}